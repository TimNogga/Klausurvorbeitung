\documentclass{report}

\input{preamble}
\input{macros}
\input{letterfonts}
\usepackage{tikz}
\newtheorem{lemma}{Lemma}
\usetikzlibrary{matrix, positioning, arrows.meta}

\title{\Huge{Algo Zusammenfassung}}
\author{\huge{Tim Nogga}}
\date{}

\begin{document}

\maketitle
\newpage% or \cleardoublepage
% \pdfbookmark[<level>]{<title>}{<dest>}
\pdfbookmark[section]{\contentsname}{toc}
\tableofcontents
\pagebreak

\chapter{Einleitung}
\section{Insertionsort}
\begin{algorithm}[H]
\KwIn{int[] a}
\KwOut{a sortiert}
\For{int j = 1; $j < a$.length; j++} {
	int x = a[j]\;
	int i = j - 1\;
	\While{$i \geq 0$ a[i] $> x$} {
		a[i + 1] = a[i]\;
		i = i - 1\;
	}
	a[i + 1] = x\;

}
\Return a;
\caption{Insertion-Sort}
\end{algorithm}
\thm {}{Die Ausgabe von Insertionsort ist stets eine aufsteigend sortierte Permutation der Eingabe.}
\thm {}{Die Laufzeit von Insertionsort ist in $O(n^2)$}

\section{Größenordnungen}
\dfn{Größenordnungen}{Seien $f,g: \mathbb{N} \rightarrow \mathbb{R}_{\geq 0}$ zwei Funktionen
\begin{enumerate}[label=\bfseries\tiny\protect\circled{\small\arabic*}]
	\item $f(n) \in \mathcal{O}(g(n))$ falls $\exists c > 0, n_0 \in \mathbb{N}$ mit $f(n) \leq c \cdot g(n) \forall n \geq n_0$
	\item $f(n) \in \Omega(g(n))$ falls $\exists c > 0, n_0 \in \mathbb{N}$ mit $f(n) \geq c \cdot g(n) \forall n \geq n_0$
	\item $f(n) \in \Theta(g(n))$ falls $f(n) \in \mathcal{O}(g(n))$ und $f(n) \in \Omega(g(n))$
	\item $f(n) \in o(g(n))$ falls $\forall c > 0 \exists n_0 \in \mathbb{N}$ mit $f(n) \leq c \cdot g(n) \forall n \geq n_0$
	\item $f(n) \in \omega(g(n))$ falls $\forall c > 0 \exists n_0 \in \mathbb{N}$ mit $f(n) \geq c \cdot g(n) \forall n \geq n_0$
	\end{enumerate}
}
\nt{Es gilt für die exakten schranken $\lim_{n\to\infty}\frac{f(n)}{g(n)}=0$, wenn $f(n)\in o(g(n))$ und umgekehrt, wenn $f(n)\in\omega(g(n))$}
\chapter{Methoden zum Entwurf von Algorithmen}
\section{Divide and Conquer}
\subsection{Binary Search}
\begin{algorithm}[H]
\KwIn{int[] a, int x, int l, int r}
\KwOut{Bool}
\If{$l > r$} {
	\Return false\;
}
int m = $\lfloor \frac{l + r}{2} \rfloor$\;
\If{$a[m] < x$} {
	\Return binarySearch(a, x, m + 1, r)\;
}
\If{$a[m] > x$} {
	\Return binarySearch(a, x, l, m - 1)\;
	
}
\caption[short]{Binary Search}
\end{algorithm}
\thm{}{Die Laufzeit von Binary Search ist in $O(\log n)$}
\nt{Erklärung:
\begin{itemize}
	\item $l > r$ bedeutet, dass das gesuchte Element nicht in der Liste ist
	\item $a[m] < x$ bedeutet, dass das gesuchte Element rechts von $m$ ist
	\item $a[m] > x$ bedeutet, dass das gesuchte Element links von $m$ ist
\end{itemize}
Geht rekurisv weiter, bis das Element gefunden ist.}
\subsection{Merge Sort}
\begin{algorithm}[H]
	\KwIn{int[] a, int l, int r}
	\KwOut{a sortiert}
	\If{$l < r$} {
		int m = $\lfloor \frac{l + r}{2} \rfloor$\;
		mergeSort(a, l, m)\;
		mergeSort(a, m + 1, r)\;
		merge(a, l, m, r)\;
	}
	\caption[short]{Merge Sort}
\end{algorithm}
\begin{algorithm}[H]
	\KwIn{int[] a, int l, int m, int r}
	\KwOut{a sortiert}
	int [] l = new int[m - l + 1]\;
	int [] r = new int[r - m]\; 
	\For{int i = 0; $i < l.length$; i++} {
		l[i] = a[l + i]\;
	}
	\For{int i = 0; $i < r.length$; i++} {
		r[i] = a[m + 1 + i]\;
	}
	int iL = 0\;
	int iR = 0\;
	int iA = l\;
	\While{iL $ < $ m-l + 1 and iR $ < $ r - m} {
		\If{l[iL] $\leq$ r[iR]} {
			a[iA] = l[iL]\;
			iL++\;
			iA++\;
		} \Else {
			a[iA] = r[iR]\;
			iR++\;
			iA++\;
		}
		\While{iL $ < $ m-l + 1} {
			a[iA] = l[iL]\;
			iL++\;
			iA++\;
		}
		\While{iR $ < $ r - m} {
			a[iA] = r[iR]\;
			iR++\;
			iA++\;
		}
	}

	\caption[short]{Merge}
\end{algorithm}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{images/IMG_86CA0912DB1F-1.jpeg}
	\caption{Mergesort}
	\label{fig:1}
\end{figure}
\thm{}{Die Laufzeit von Merge Sort ist in $O(n \log n)$}
\subsection{Divide-and-Conquer am Beispiel von Strassen}
\nt{Wenn man die Laufzeit von Strassen zu Simpleren Divide and Conquer Algorithmen vergleicht, wie Simple Product, ist die Laufzeit von Strassen besser}
\thm{}{Die Laufzeit von Strassen ist in $O(n^{\log_2 7})$}
\begin{myproof}
	Siehe Seite 25 skript(Kein Bock das zu Texen) folgt aber aus $T(n) = 7T(n/2) + O(n^2)$
\end{myproof}
\subsection{Master Theorem}
\thm{Mastertheorem}{Seien $a \geq 1, b > 1,$ Konstanten und sei f: $\mathbb{N} \to \mathbb{R}_{\geq 0}$ eine Funktion. Ferner sei $T: \mathbb{N} \to \mathbb{R}$ definiert durch $ T(1) = \Theta(1)$ und $$ T(n) = aT(\frac{n}{b}) + f(n)$$ für alle $n > 1$.
Die Funktion T kann wie folgt beschrieben werden:
\begin{itemize}
	\item $T(n) = O(n^{\log_b a})$ falls $f(n) = O(n^{\log_b a - \epsilon})$ für eine Konstante $\epsilon > 0$
	\item $T(n) = \Theta(n^{\log_b a} \log n)$ falls $f(n) = \Theta(n^{\log_b a})$
	\item Falls $T(n) = \Omega(n^{\log_b a + \epsilon})$ für eine Konstante $\epsilon > 0$ und falls $af(\frac{n}{b}) \leq cf(n)$ für eine Konstante $c < 1$ und alle hinreichend großen $n$, dann gilt $T(n) = \Theta(f(n))$
\end{itemize}
}
\nt{Im Prinzip werden nur die Funktionen n und $n^{\log_{b}a}$ verglichen. Im ersten Fall wächst f langsamer im zweiten gleich schnell. Verglichen zum ersten Fall führt das zu einem zusätzlichen $\log n$ führt. Im dritten Fall wächst f schneller als $n^{\log_{b}a}$, also ist die Lösung $\Theta(f(n))$}
\begin{lemma}
	Seien $a \geq 1, b > 1,$ Konstanten und sei f: $\mathbb{N} \to \mathbb{R}_{\geq 0}$ eine Funktion. Sei $T(n) = \Theta(O(n^{\log_{b}(a)})) + g(n)$ mit $g(n) \defeq \sum_{i=0}^{\log_{b}(n) - 1} a^{i}f(\frac{n}{b^{i}})$ Betrachte die Funktion beschränkt auf Werte von n mit $n = b^{j} \text{ für } j \in \mathbb{N}$.
	\begin{itemize}
		\item Falls f(n) = O($n^{\log_{b}(a) - \epsilon}$) für eine Konstante $\epsilon > 0$, dann $g(n) = O(n^{\log_{b}(a)})$
		\item Falls f(n) = $\Theta(n^{\log_{b}(a)})$, dann $g(n) = \Theta(n^{\log_{b}(a)} \log n)$
		\item Falls f(n) = $\Omega(n^{\log_{b}(a) + \epsilon})$ für eine Konstante $\epsilon > 0$ und falls $af(\frac{n}{b}) \leq cf(n)$ für eine Konstante $c < 1$ und alle hinreichend großen $n$, dann gilt $g(n) = \Theta(f(n))$
	\end{itemize}
\end{lemma}
\nt{Die Funktion g(n) ist die Summe der Kosten der rekursiven Aufrufe.\\ 
	Das Lemma beschreibt das Mastertheorem für den Fall, dass n eine Potenz von b ist. Die höhe des Rekursionbaumes beträgt $\log_{b}(n)$ Logischerweise ist die gesammte Laufzeit die Summe über alle Knoten für jedes Level, wobei sich die anzahl an Knoten mit der höhe Skaliert mit $a^{h}$ hinzu kommt dannn noch das die Kosten pro level noch mit der Summe $\sum_{i=0}^{h-1} f(\frac{n}{b^{i}})$ gegeben ist. Also ensteht daher die Formel für die gesammt Laufzeit mit $\Theta(a^{h}) + \sum_{i=0}^{h-1} f(\frac{n}{b^{i}})$}
\begin{figure}[h]
	\centering
	\includegraphics[width=0.8\textwidth]{images/Mastertheorem.jpeg}
	\caption{Visualized Lemma 1}
	\label{fig:2}
\end{figure}
\section{Greedy Algorithmen}
\dfn{Greedy Algorithmen}{Greedy Algorithmen sind Algorithmen die immer den besten lokalen Schritt wählen. }
\ex{Wechselgeldproblem}{Das Wechselgeldproblem stellt die Münzen schritt für schritt zusammen, die am nächsten an den Restbetrag herankommen.
Wie viel noch fehlt wird sich in der Variable $z'$ gemerkt.}
\thm{}{Der Greedy Algorithmus löst das Wechselgeldproblem optimal.(Was ist wenn die Währung doof ist, siehe nächste Note)}
\nt{Für richtige Währungen wenn man Quatsch Währungen wählt lässt sich schnell ein gegenbeispiel Konstruieren z.B. 1,3,4 jezt 6 als Betrag. Also wird halt die 4 gewählt weil die am größten ist dementsprechend wird in den nächsten 2 Schritten 2 mal die 1 gewählt, aber die 3 und 3 wären besser gewesen.}
Ich mach einfach mal nen beweis für die Vibes dazu, keine Ahnung gerade Bock drauf steht aber auch so im Skript(in Schöner siehe Seite 29).
\begin{proof}
	Sei $z \in \mathbb{N}$ ein beliebiger Betrag, welcher genau erreicht werden soll. Für $i \in M : = \{1,2,5,10,20,50,100,200\}$ mit $x_{i} \in \mathbb{N_{0}}$
	Wegen der Def von Greedy Algorithmen, das immer der Lokal beste Schritt gewählt wird folgt das die ungleichung mit $i \in M, i > 1$ gilt 
	$$ \sum_{j \in M, j < i}^{}jx_{j} < i  $$
	Das j ist hierbei die Münze die gerade betrachtet wird, diese ist immer kleiner als i, da sonst das i gewählt werden würde, was gegen die Defenition von Greedy Algorithmen verstößt, da es eine bessere Lösung gibt.\\
	Zusammen mit der Ausgangsbedingung das der zu erreichende Betrag z immer $\sum_{i \in M}^{}ix_{i}$ ist. Nun lässt sich hierdrüber folgende Induktion aufbauen.\\
	 $z = 1$ trivialerweise gilt die Aussage.\\ Die Aussage gilt für alle $z \in \mathbb{N}$ mit $z > 1$ betrachte $$ \sum_{j \in M, j < i}^{}jx_{j} < i  $$ \\ für ein $i \ maximal \leq z$ muss mindestens eine Münze vom Wert i enthalten sein, da sonst der Betrag nicht erreicht werden kann.\\
	Die Behauptung folgt auch für $z' = z - i$ da $z' \leq z$ gilt.\\ 
	Also ist die Lösung vom Greedy Algorithmus mit der Ungleichung erfüllt.\\
	Nun lässt sich Annehmen, das sich eine optimale Lösung finden lässt. Wenn man alle Münzen in M durchgeht lässt sich immer jede Münze durch eine Kombination von Münzen mit kleinerem Wert ersetzen.\\
	Formaler sei $y_{i} \in \mathbb{N}_{0}$ mit $ \sum_{i \in M} iy_{i} = z $ und kleinstmöglicher Zahl $\sum{i \in M} y_{i}$\\
	Sei die Lösung in Optimalerweise $x_{i} \in \mathbb{N}_{0}$ Zum Beispiel gilt $y_{1} \leq 1$, da zwei 1 Cent Münzen durch eine 2 Cent Münze ersetzt werden können.\\
	Daraus folgt die ungleichung $1 \cdot y_{1} < 2$\\
	Analog folgt für $y_{2}, y_{2} \leq 2$, da sich 3 2 Cent Münzen durch eine 5 Cent Münze und eine 1 Cent Münze ersetzen lassen.\\
	Daraus folgt die ungleichung $1 \cdot y_{1} + 2 \cdot y_{2} < 5$\\ $\rightarrow$ Dies gilt für alle $i \in M$
\end{proof} 
\subsection{Optimale Auswahl von Aufgaben}
\nt{Intervall Scheduling:
Jede Aufgabe hat eine Startzeit $s_{i} \geq 0$ und eine Endzeit $f_{i}$, wobei $s_{i} < f_{i}$ gilt.\\
Zudem steht ein Prozesor zur verfügung der nur eine Aufgabe gleichzeitig bearbeiten kann.\\
Die Aufgabe ist es nun eine möglichst große Teilmenge von Aufgaben zu finden, die sich nicht überschneiden.\\}
\dfn{Intervall Scheduling Formales Ziel}{Gescucht ist eine Teilmenge $S' \subseteq S$, sodass für jedes $i,j \in S'$ gilt $i \neq j$ und $[s_{i}, f_{i}) \cap [s_{j}, f_{j}) = \emptyset$}
\nt{Es liegt nahe das es einen Greedy Algorithmus gibt welcher das Problem löst(Im Skript kommen noch Algorithmen die nicht klappen)}
\begin{algorithm}[H]
\caption{Greedy Ende}
$S^{*}$ = $\emptyset$\\
\While{$S \neq \emptyset$}{
	\text{Wähle die Aufgabe i mit dem frühesten Ende}\\
	$S^{*} = S^{*} \cup \{i\}$\\
	$S = S \setminus \{i\}$ \text{Keine Ahnung nicht im Skript aber kommt mir logisch vor}\\ 
	\text{Entferne alle Aufgaben die sich mit i überschneiden}

}
\end{algorithm}
Für den Beweis von den Algorithmus wird folgendes Lemma benötigt.
\mlenma{}{Es sei S eine Menge von Aufgaben und es sei $i \in S$ eine Aufgabe mit dem frühesten Ende. Dann gibt es eine Optimale Auswahl $S' \subseteq S$ von paarweise nicht kollidierenden Aufgaben mit $i \in S'$}
\nt{Das Lenma sagt basically nur aus das es eine optimale Lösung gibt.}
\begin{proof}
	Genauer Beweis im Skript seite 32. Aber man definiert sich eine Optimale Menge $S^{*}$ in diese fügt man dann eine Aufgabe i ein. Die Menge $S^{*}$ ist dann immer noch optimal und es gibt keine Überschneidungen, da i vor oder gleich mit j der vorher kürzesten Aufgabe aufhört. j hat sich halt obviously auch nicht überschnitten, weil dann wär es nicht optimal gewesen. 
\end{proof}
\thm{}{Der Algorithmus GreedyEnde wählt für jede Instanz eine größtmög-
liche Menge von paarweise nicht kollidierenden Aufgaben aus.}
\begin{proof}
	Lässt sich recht entspannt über ne invariante Zeigen:\\
	In Zeile 2 gilt immer das $S^{*}$ sich zu einer optimalen Lösung erweitert.\\
	In der ersten Iteration gilt die Invariante, da \( S^* \) leer ist und \( S \) noch alle Aufgaben enthält. Sei nun die Invariante zu Beginn eines Schleifendurchlaufs erfüllt. Dann gibt es eine optimale Auswahl \( \hat{S} \subseteq S^* \cup S \) von Aufgaben, die alle Aufgaben aus \( S^* \) und gegebenenfalls zusätzliche Aufgaben aus \( S \) enthält. Außerdem kollidieren Aufgaben aus \( S^* \) nicht mit Aufgaben aus \( S \). Das heißt, bei der Frage, welche Aufgaben aus \( S \) den Aufgaben aus \( S^* \) noch hinzugefügt werden müssen, um eine optimale Auswahl \( \hat{S} \) zu erhalten, spielen die Aufgaben aus \( S^* \) keine Rolle. Die Menge \( \hat{S} \setminus S^* \) dieser Aufgaben ist eine größtmögliche Teilmenge von paarweise nicht kollidierenden Aufgaben aus \( S \). Gemäß Lenma 2.2.1 existiert eine solche Menge, die die Aufgabe \( i \in S \) mit dem frühesten Fertigstellungszeitpunkt \( f_i \) aller Aufgaben aus \( S \) enthält. Genau diese Aufgabe fügt der Greedy-Algorithmus am Ende der Menge \( S^* \) hinzu. Anschließend werden aus \( S \) alle Aufgaben entfernt, die mit dieser Aufgabe \( i \) kollidieren. Dies garantiert zusammen, dass auch am Anfang der nächsten Iteration die Invariante wieder erfüllt ist.
\end{proof}
\dfn{Laufzeit Greedy Ende}{Die Laufzeit des Algorithmus GreedyEnde beträgt $O(n \log n)$, wobei $n = |S|$ die Anzahl der Aufgaben in der Eingabe bezeichnet. Sind die Aufgaben bereits aufsteigend nach ihrem Fertigstellungszeitpunkt sortiert, so beträgt die Laufzeit O(n).}
\nt{Es läuft halt n mal durch und man kann es in $O(n \log n)$ sortieren.}
\subsection{Rucksackproblem mit Teilbaren Aufgaben}
Das Rucksack, wie wir es Behandeln (anscheinend kommt mehr in Algo 2) besteht aus einer Menge von Objekten $O = \{1, \dots, n\}$, wobei jedes Objekt $i \in O$ ein Gewicht $w_{i} \in \mathbb{N}$ und einen Nutzen $p_{i} \in \mathbb{N}$ besitzt.\\
Es soll eine Teilmenge der Objekte gefunden werden, die in den Rucksack passt und unter dieser Bedingung maximalen Nutzen besitzt.
Jetzt wo es ein bisschen anders ist: \\ Wir dürfen das Objekt Teilen für den maximalen Nutzen.

\begin{algorithm}[H]
\caption{Greedy Rucksack}
Sortiere die Objekte nach dem Nutzen pro Gewicht $p_{i}/w_{i}$ in absteigender Reihenfolge.\\
\For{int i = 1; i $\leq$ n; i++}{x[i] = 0;}
int i = 1;\\
\While{$t>0$ and $i\leq n$}{
	\If{$t \geq w_{i}$}{
		$x_{i} = 1$\\
		$t = t - w_{i}$
	}
	\Else{x[i] = t/w[i], t = 0\\}
	i++;
}
\Return{$x_{1}, \dots, x_{n}$}
\end{algorithm}
\thm{}{Der Algorithmus GreedyRucksack liefert für jede Instanz eine optimale Lösung in $O(n \log n)$ Zeit.}
\begin{proof}
	Zur Laufzeit: Greedy Rucksack wird von den Sortieren Dominiert, dies läuft über Merge-Sort in $O(n \log n)$. Die Schleifen können jeweils nur n mal durchlaufen, was die laufzeit von $O(n \log n)$ zeigt.\\
	Zur Korrektheit: Da die Lösung trivial ist, wenn alle Objekte in den Rucksack passen wird im folgenden nur der andere Fall betrachtet mit $t < \sum_{i=1}^{n} w_{i}$.\\
	Es gibt nun ein $i \in \{1, \dots, n\}$ mit $x_{1} = \dots = x_{i-1} = 0 $ $x < 1 $ und $x_{i+1} = \dots = x_{n} = 0$\\
	Greedy Rucksack füllt natürlich auch den Rucksack. Jede Optimale Lösung muss den Rucksack auch ausfüllen, das heißt es gilt $\sum_{i=1}^{n} x_{i}^{*} w_{i} = t$.\\
	Es wird nun gezeigt, dass sich die Lösung von $x^{*}$ in die Lösung x von Greedy Rucksack umwandeln lässt ohne die Nutzen zur veringern.\\
	Jetzt wird der kleinste Index i genommen $x_{i}^{*} < 1$ und $x_{i+1}^{*} = \dots = x_{n}^{*} = 0$. Hier gilt jetzt $x^{*} = x$ muss mir nochmal anschauen warum.TODO
	Es gibt einen Index $j > i$ mit $x_{j}^{*} > 0$. Sei j der größte solcher Indexe. Die j lassen sich jetzt reduzieren während man das i gegenscaled. Wegen der Effizienz von i welche mindestens die von j ist verändert sich nicht der Nutzen
\end{proof}

\end{document}
